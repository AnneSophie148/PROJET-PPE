{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba    \n",
    "# module for Chinese word segmentation \n",
    "\n",
    "fR = open('./itrameur/contextes-ch.txt', 'r', encoding='UTF-8')         \n",
    "line = fR.read()        \n",
    "seg_list = jieba.cut(line)      \n",
    "# open & read file for tokenization -> tokenize line by line\n",
    "\n",
    "fW = open('./itrameur/contextes-ch_seg.txt', 'w', encoding='UTF-8')      \n",
    "fW.write(' '.join(seg_list))        \n",
    "# put result in new file, separate tokens by space\n",
    "\n",
    "fR.close()\n",
    "fW.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "text = open('./itrameur/contextes-ch_seg.txt').read()\n",
    "\n",
    "wc = WordCloud(font_path='/Users/ko/Library/Fonts/SourceHanSerif.ttc')\n",
    "\n",
    "wordcloud.generate(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# script du prof\n",
    "\"\"\"\n",
    "import thulac\n",
    "import errno\n",
    "import fileinput\n",
    "\n",
    "# autre possibilité, lancer la commande:\n",
    "# python -m thulac chinois.txt chinois_seg_output.txt -seg_only\n",
    "# Mais ne permet pas les redirections d'entrées/sorties\n",
    "\n",
    "seg = thulac.thulac(seg_only=True)\n",
    "try:\n",
    "    for line in fileinput.input(files='chinois.txt'):\n",
    "        print(seg.cut(line, text=True))\n",
    "except IOError as e:\n",
    "    if e.errno != errno.EPIPE:\n",
    "        raise\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = './itrameur/contextes-eng.txt'\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "filtered_text = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "with open('./contextes-en_filtered.txt', 'w', encoding='UTF-8') as f:\n",
    "    f.write(str(filtered_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit ('3.10.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "189267f75c10e6644f71465dc1cf5757dd3130fc4f4cb39e75db07c8876d2fda"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
